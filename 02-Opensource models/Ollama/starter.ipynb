{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99ca456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.35.10)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai) (2.8.1)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (4.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac947de-effd-4b61-8792-a6d7a133f347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7a1867e28220>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import minsearch\n",
    "\n",
    "with open(\"documents.json\",\"r\") as docs:\n",
    "    documents_raw = json.load(docs)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f087272-b44d-4738-9ea2-175ec63a058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90eb4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    # required but ignored\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "742ab881-499a-4675-83c4-2013ea1377b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='phi3',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8bff3e-b672-42be-866b-f2d9bb217106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46457332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, you can run dbt locally using Docker and Postgres as per the provided context. Here are detailed steps based on that information:\\n\\n1) First, ensure your postgres instance is running by referencing week 2 of year 2024 docker-compose setup from FAQs database section \"Project.\" You should find a link or reference to it in there which you could follow if needed. The exact details are not mentioned but this gives the direction that one already has access to such resources as per context.\\n   \\n2) In your local machine, create a directory `dbt` and navigate into: \\n   ```shell\\n   mkdir dbt && cd dbt\\n   ```\\n3) Clone the needed repository with git command by replacing `<your-path>` in this line from FAQs database section \"Project\":\\n   ```shell\\n   vi dbt/profiles.yml  # Edit as required and then save it using Ctrl + X or right click -> Save\"\\n   \\n4) Navigate to the `dbt` folder, create a project directory inside of it:\\n   ```shell0 bash -c \"cd ../project && mv dbt-starter-project/* .\"\\n``` \\n5) Align profile name in your local \\'profiles.yml\\' with that mentioned on GitHub under section Project as per FAQs database and replace the `dbt_project.yml` configuration version to at least v2:\\n   ```yaml\\n   config-version: 2\\n   ```\\n6) To run dbt locally, set Docker variables correctly pointing towards your `.env` file which was created for week 2 of year 2024 in the docker context and execute this command from FAQs database section \"Project\":\\n   ```shell\\n   docker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app \\\\ \\n      --mount type=bind,source=/<your-path>,target=/root/.dbt bash dbt ls -qe > output.log && tail +500 output.log | grep \\'run\\' >> run_output.txt  \\n   ```   \\n7) If you encounter any trouble during running the command above use `debug` mode: \\n   ```shell\\n   docker run --network=mage-zoomcamp_default \\\\\\n      --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app \\\\\\n      --mount type=bind,source=/<your-path>,target=/root/.dbt bash dbt debug \\n   ```   \\n8) When making changes in production or trying to deploy your project on a different location as per the context \"Module 4: analytics engineering with dbt,\" ensure you have done all necessary prerequisites mentioned, including merging branches and ensuring datasets are correctly named. If required error occurred due to wrong dataset names for example, rename it in BigQuery matching production environment\\'s name or recreate it following proper procedures as suggested on context \"Module 4: analytics engineering with dbt.\"\\n   \\n9) Finally if you run into a build error specifically related to missing configuration files check your local setup and ensure the `dbt_project.yml` file is in place, pointing towards valid database credentials as per FAQs\\' section on Dashboard of \"Module 4: analytics engineering with dbt.\"\\n   \\nRemember always verify each step before moving onto next to avoid errors or misalignments which might occur due to various factors like environment setup discrepancies.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"can i run dbt locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ec2b773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, you can run dbt locally using Docker and Postgres as per the provided context. Here are detailed steps based on that information:\n",
      "\n",
      "1) First, ensure your postgres instance is running by referencing week 2 of year 2024 docker-compose setup from FAQs database section \"Project.\" You should find a link or reference to it in there which you could follow if needed. The exact details are not mentioned but this gives the direction that one already has access to such resources as per context.\n",
      "   \n",
      "2) In your local machine, create a directory `dbt` and navigate into: \n",
      "   ```shell\n",
      "   mkdir dbt && cd dbt\n",
      "   ```\n",
      "3) Clone the needed repository with git command by replacing `<your-path>` in this line from FAQs database section \"Project\":\n",
      "   ```shell\n",
      "   vi dbt/profiles.yml  # Edit as required and then save it using Ctrl + X or right click -> Save\"\n",
      "   \n",
      "4) Navigate to the `dbt` folder, create a project directory inside of it:\n",
      "   ```shell0 bash -c \"cd ../project && mv dbt-starter-project/* .\"\n",
      "``` \n",
      "5) Align profile name in your local 'profiles.yml' with that mentioned on GitHub under section Project as per FAQs database and replace the `dbt_project.yml` configuration version to at least v2:\n",
      "   ```yaml\n",
      "   config-version: 2\n",
      "   ```\n",
      "6) To run dbt locally, set Docker variables correctly pointing towards your `.env` file which was created for week 2 of year 2024 in the docker context and execute this command from FAQs database section \"Project\":\n",
      "   ```shell\n",
      "   docker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app \\ \n",
      "      --mount type=bind,source=/<your-path>,target=/root/.dbt bash dbt ls -qe > output.log && tail +500 output.log | grep 'run' >> run_output.txt  \n",
      "   ```   \n",
      "7) If you encounter any trouble during running the command above use `debug` mode: \n",
      "   ```shell\n",
      "   docker run --network=mage-zoomcamp_default \\\n",
      "      --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app \\\n",
      "      --mount type=bind,source=/<your-path>,target=/root/.dbt bash dbt debug \n",
      "   ```   \n",
      "8) When making changes in production or trying to deploy your project on a different location as per the context \"Module 4: analytics engineering with dbt,\" ensure you have done all necessary prerequisites mentioned, including merging branches and ensuring datasets are correctly named. If required error occurred due to wrong dataset names for example, rename it in BigQuery matching production environment's name or recreate it following proper procedures as suggested on context \"Module 4: analytics engineering with dbt.\"\n",
      "   \n",
      "9) Finally if you run into a build error specifically related to missing configuration files check your local setup and ensure the `dbt_project.yml` file is in place, pointing towards valid database credentials as per FAQs' section on Dashboard of \"Module 4: analytics engineering with dbt.\"\n",
      "   \n",
      "Remember always verify each step before moving onto next to avoid errors or misalignments which might occur due to various factors like environment setup discrepancies.\n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8376cd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
